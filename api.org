#+title: Api
Planning for the API, data model etc.
* Questions
** General Implementation
How stable is the data structure, i.e how likely is DESI to change/extend the file structure in the near future?

My understanding is that there's no underlying database (i.e SQL or anything), it's just flat FITS files stored in a highly nested/structured manner, and each FITS file is basically a collection of tables with standard format/structure and some metadata attached to each table, and we use our `Spectra' object as an abstraction around this. Is that correct, or is there anything I'm misunderstanding?
** RADEC Endpoint
Off the top of my head, the most obvious way to implement it is to read in all of the rows from =zall-pix*.fits= (selecting only the =TARGET_ID, TARGET_RA, TARGET_DEC= columns) and keep rows where the distance to the requested =(RA, DEC)= is less than =RADIUS_ARCSEC=, and then hand it off to a subroutine to get spectra for each target id.

I'm still getting a handle on the scale of the data involved here, so I wanted to ask if reading and filtering all the rows from =zall-pix*.fits= (including running basic arithmetic on them as part of the filter) is feasible from a performance perspective? If not, we can try to think of more effective ways of handling this endpoint.

** Underlying abstraction
As I understand it every API response looks like a file derived from a `Spectra' object - either an image file with a plot of the spectra, or a FITS file containing the data from the spectra.
Is my understanding correct, and also is this likely to remain true as we extend the API?
If it is, then I think we can separate the code into
a) a top-level layer which accepts API requests, calls a subroutine to build a Spectra object, translates that object to a file of some kind (based on whether the request was download/plot) and sends back the resulting file over the network
b) a module that uses desispec utils to do file reading + slicing/selection, including stuff like mapping edr -> fuji. Basically, most knowledge of the DESI data model lives here.
c) a module to do complicated work with Spectra objects.
** Extensibility
Currently the flow looks like: someone accesses one of the three endpoints we specified -> we give them back either a FITS file or an image file.

At a high level, any other major features we think it's important to build into the API? For instance, some kind of web viewer for the plots, authentication, etc.?
Obviously I'll make sure the implementation is as extensible as possible, but if there are concrete extensions we want to accomodate I'll plan explicitly for those.
** Cacheing and Cleanup
Since our responses currently look like files (whether FITS or HTML or image), I was imagining we could save each response file to =$CACHE_DIR/<api-call-parameters>/<request timestamp>=.
That way when a new API call is made, we can just:
- Check if something exists at the relevant path. If not, proceed with building the file.
- Check if the existing thing is sufficiently recent (we can define an arbitrary cutoff). If not, delete the old file and proceed with building the new one.
We can also have a low-priority cron job to just =rm -rf $CACHE_DIR= run however often we want.
** Limiting Response Size
One way to do this is pagination, although this might be tedious since our data is FITS files and I'm not sure how `naturally' they paginate.

If we know the typical size of the data, we can just set a maximum for request parameters like the size of the =radius= param or the number of target ids, and add a validation check to send back a descriptive error on invalid requests.

* Endpoints
All endpoints specify a release first like `daily' or the special name of a release.
** Tile
=tile/<tile-id>/<fibers>=
Select a tile folder, latest date, extract the relevant PETAL file names based on the <fibers>. Read each file into a Spectra, keep the ones matching requested fibers, and =stack= them.
# TODO: Look at the structure of FITS files and petal files specifically. So do we need to flat-concat the files, or can we do clever things with pulling out individual tables.

** Targets
=targets/<target-ids>=
Read a FITS to get the Healpix, Survey, Program for each target. That is sufficient to get a file path to a FITS file for each target

Then, simply read each file using read_spectra, restrict to the relevant target_ids, and stack them all.
Do some grouping so targets with the same file are pulled in a single read

Some intricacies - the filename depends on the release sometimes, it seems. Annoying.
** Radec
=radec/<ra,dec,radius>=
So select a point in the sky and a radius around it.
The zall-pix-<release>.fits is the core metadata, it seems.
This one seems iffy, geometry required, some actual algorithms.

So we are given a point and radius, we are interested in all targets within that radius.

* Methods
** Download
** Plot
* Overview
The ideal separation looks like:
** Assemble the data
*** Find the right files
*** Implementation knowledge
What parts of what files to extract.
This is just a set of calls to =read_spectra= to extract relevant rows and blindly pass them on.

**** FITS files - select columns
**** Queries to apply to get rows
Applied in =read_spectra=
*** Consolidate
- Execute the `query' assembled by the domain knowledge
- Builds a Spectra object


** Download
- Write the spectra to a tmp file
** Plot
- Write the plot to a tmp file
** Serve
- Takes a file path and sends it back over the wire.
-
** Top-level
Top-level routine.
- Accepts params
- Checks cache for existing
- Save timestamp of request
- Dispatch to utility to actually build spectra
- Takes returned spectra. Either build a fits file or a plot.png from it, and put it in the right place in /cache
- Return the filename
* Details
** Error-handing
* Sqlite DB
- Stephen's suggestion is to maintain a sqlite DB that reflects the zall.fits for each `frozen' release.
- How would this work?
** Requirements
- Build a DB
- Mechanically, use python's internal model of fits as our intermediate
- Can we cheese? https://github.com/noaodatalab-user/fits2db
- Damn, last commit 2018. Maybe not.
- Possibly filter cols/rows
- Rebuild on-demand
- Periodically sync/sanity check
- Generic: Build from new releases as well.
- Ideally: Automated pipeline to build but that's far-future
- Fallback - if this fails or is ambiguous somehow, try to read straight from FITS
- Encapsulate the fallback - the actual endpoint handler should just call `get-stuff-from-zall' and have that do everything it can.
-
* TODO
- Set up venv
- Review tutorial
-


* Testing on NERSC
- Just run from the Jupyter notebook.
- Should have DESI access from my notebook
- They have SSH access.
- So ssh in to NERSC, clone the git repo I'm working with
- Also set up emacs CLI in there. Emacs is on, clone dotfiles, install doom
- Jupyterhub lets you do terminals as well, but eww.
* Parameter parsing
** Reading
- There have to be standards for this, come on.

** Ideas
- Define a ParameterObject union type?
- Union of one dataclass per endpoint.
- Empty class =ParameterObject=
- Subclasses for actual content
* Notes from the Slice/Dice
- read_spectra(filename)
- write_spectra(outfile, spectra)
- .num_spectra to check size
- desispec.io.find_file


We can do NP-style complex logical filters and maps
#+begin_src python
keep = spectra.fibermap['FIBER'] % 50 == 0
subset1 = spectra[keep]

focalplane_radius = np.sqrt(subset1.fibermap['MEAN_FIBER_X']**2 + subset1.fibermap['MEAN_FIBER_Y']**2)
subset1.fibermap['FOCALPLANE_RADIUS'] = focalplane_radius
#+end_src

#+begin_src python
zcat = fitsio.read(zcatfile, 'ZCATALOG', columns=('TARGETID', 'TILEID', 'LASTNIGHT', 'PETAL_LOC', 'SPECTYPE', 'Z', 'ZWARN', 'FLUX_G'))

for tileid, night, petal in np.unique(bright_qso_zcat['TILEID', 'LASTNIGHT', 'PETAL_LOC']):
    coaddfile = desispec.io.findfile('coadd', tile=tileid, night=night, spectrograph=petal,
                                     groupname='cumulative')
    spectra = desispec.io.read_spectra(coaddfile, targetids=bright_qso_zcat['TARGETID'])
    spectra_camcoadd = coadd_cameras(spectra)
    spectra_list.append(spectra_camcoadd)

#+end_src
* Docs
#+begin_src python
desispec.io.meta.findfile(filetype, night=None, expid=None, camera=None, tile=None, groupname=None, healpix=None, nside=64, band=None, spectrograph=None, survey=None, faprogram=None, rawdata_dir=None, specprod_dir=None, download=False, outdir=None, qaprod_dir=None, return_exists=False, readonly=False, logfile=False)[source]Â¶
#+end_src

* TODO <2023-10-18 Wed>
** Questions
- Should we do partial cacheing? No, we should not, it is simply not worth the effort.
- Overhead of reading RA/DEC?
*** How do we catch/log/report errors?
- No comments on webapp
- Inside the spectra module: Fail early, raise an exception. So looks go-style. So raise as early as possible, as informative as possible.
- For webapp: Send an error response to user, log it to stdout, raise an exception. Wrap this all in a function, somehow.
- Have  a func that does logging/etc. and returns an exception, and then =raise= that.

*** How to read Daily, without a zcatalog? Use the csv?
- Think about this later
- Look into sqlite
- RADEC can be done from csv kind of.
** Radec
- In principle I have a working impl, test it.
** Error-handling
- Catch user errors carefully
- For internal errors, have catches in the top-level handlers in the relevant module
- Catch errors, log them, respond with a traceback
** Testing - edge cases
*** How to interpret outputs for testing:
- Trivial: Check if it's pulled out the correct line
- Sanity-check unit tests: Run without crashing, produce nonempty files, etc.
- Do low-bar testing: Does it return any rows, etc.
- So do manual testing to check content, on bug write unit test that will catch it if it regresses, or anything.
- So catch breaking changes early
** Testing - more than sanity
** Webapp side
- All of this, really
** Sqlite idea
** Dockerization stuff
** Daily
** Optimisation
- Don't read the same file multiple times
- Ask: Overhead of reading RA/DEC?
** Cacheing
Should we do partial cacheing? No, we should not, it is simply not worth the effort.
* TODO <2023-10-19 Thu>
- Work on a user guide, send to Stephen over email and add to github.
- Corollary
- Next steps: RADEC + webapp
- Webapp testing:
* Security Design
- Ask Stephen about existing setups.
** File Access
Same docker image.
Have one running with DESIROOT mounted, and another running with DESIROOT/fuji, DESIROOT/iron only (so access to only public specprods).
Higher abstraction level: Python script to read from a text file of public release names and build a docker command out of it and spawn that process.
** Password Access
- By default, the script runs basic-auth required
- If we can do this, we can also do allowed-releases here and avoid running different docker images.
- So then we have a private one running with password-locked = True, and a public one with password-locked = False and releases set to public.
*** Config file
- DESIROOT location
- Allowed releases?
- Password locked: y/n
- Username and password (encoded/encrypted, will look into this)
- The file will be bundled into the docker image, so not something directly accessible from the filesystem.
* Security Design Email
Firstly, I wanted to ask about the existing setup - Anthony mentioned DESI had some existing stuff in place for handling security of private/public releases, and I wanted to ask if you could direct me to any docs/wikis which go over those, or anything in particular you think I should understand about them.

I was thinking of using Basic Auth for the password-protection, since it plays well with both automated access via API and manual via webpage.
It's similar to the screening used here: https://desi.lbl.gov/desipub/app/MembershipForm/form
We'd probably store an encrypted version of the password as storage somewhere in the docker image, and check the input password against that as our auth method. Flask has various add-ons and features that should make basic auth clean to implement (I found several tutorials online).
I'm not an expert on security stuff so I'm not sure if that meets whatever standards/expectations DESI has - if it doesn't, or if you have suggestions on what to do instead, let me know.
https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication#basic_authentication_scheme


One idea was to have a config file bundled in with the docker image that dictates whether the API requires username/password (and if so, includes the credentials encrypted somehow), as well as which releases it's allowed to access.
So then you'd have two copies of the image running - one with password protection and full file access, and another with no password protection and access only to public directories.

Another idea was to just have the config file handle password protection, and restrict file access by mounting different files. When we run the docker image we can mount parts of the local filesystem - the unrestricted API would mount DESIROOT, but the restricted one might only mount DESIROOT/fuji and DESIROOT/iron, for instance.
So then the `public' docker image has no idea the private files even exist, so any attempt to access them via a bug or anything will just fail with `file not found'.
We could build higher levels of abstraction over this, like a small script to generate the correct docker commands based on a list of public releases.
The advantage here is that the main codebase doesn't have to care about whether it's allowed to access a file, so we can separate out the access control into this layer, at the cost of giving us a new moving part to worry about.
* HW <2023-11-02 Thu>
- Check refactors, fix the ones that will be big problems later if any
- Focus: Build out webapp and test, including cacheing.
- Clone fuji
- Auth/etc. doesn't matter here. So this is just a dumb API server.
- Error handling
- Consider making the Parameters structs into plain dictionaries, so we can kwarg them. Do this
- Status: Tile endpoint works, produces a file, ran rudimentary cache checks.
- Next steps: More testing, plot
* Next Steps
** Error-handling
Build_Request and Build_Params need rich error-checking so we can feed info on malformed reqs back to user.

Catch errors on top level. Errors in build_request give 403, errors elsewhere give 500.

build_response: Check if the time-parsing fails.
Check if library calls fail, basically - write_spectra and plotspectra.
Also read_tile_spectra, fitsio.read,  read_spectra, Table.read

DONE Log reqs as they come in, and log meta like rebuilding/used cache
** Validation
Largely tedium. Validate parameters as specified
TODO ask about validation rules.
** Refactoring
* Actual dockering
- Mount DESIROOT somewhere
- Specify where DESIROOT is mounted (from POV of the image)
- Might just hardcode DESIROOT='/desiroot' in the python thing, and always mount $DESIROOT to target '/desiroot' in the run.sh
- Then we escape environment var juggling
** How does this play with the public multi-mount?
Mount the allowed releases as '/desiroot/<fuji>', etc.
It works fine.
* Resp
- They would all have a similar pattern of turning the request url into formal parameters and then calling build_spectra.get_target_spectra, figured it made sense to do that in one func rather than 3.
- The basic idea is that then the webapp would have to care about this domain stuff, whereas ideally I think domain stuff will be concentrated in build_spectra.
- Will make the env var changes.
