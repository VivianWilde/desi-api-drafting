#+title: Api
Planning for the API, data model etc.
* Questions
** General Implementation
How stable is the data structure, i.e how likely is DESI to change/extend the file structure in the near future?

My understanding is that there's no underlying database (i.e SQL or anything), it's just flat FITS files stored in a highly nested/structured manner, and each FITS file is basically a collection of tables with standard format/structure and some metadata attached to each table, and we use our `Spectra' object as an abstraction around this. Is that correct, or is there anything I'm misunderstanding?
** RADEC Endpoint
Off the top of my head, the most obvious way to implement it is to read in all of the rows from =zall-pix*.fits= (selecting only the =TARGET_ID, TARGET_RA, TARGET_DEC= columns) and keep rows where the distance to the requested =(RA, DEC)= is less than =RADIUS_ARCSEC=, and then hand it off to a subroutine to get spectra for each target id.

I'm still getting a handle on the scale of the data involved here, so I wanted to ask if reading and filtering all the rows from =zall-pix*.fits= (including running basic arithmetic on them as part of the filter) is feasible from a performance perspective? If not, we can try to think of more effective ways of handling this endpoint.

** Underlying abstraction
As I understand it every API response looks like a file derived from a `Spectra' object - either an image file with a plot of the spectra, or a FITS file containing the data from the spectra.
Is my understanding correct, and also is this likely to remain true as we extend the API?
If it is, then I think we can separate the code into
a) a top-level layer which accepts API requests, calls a subroutine to build a Spectra object, translates that object to a file of some kind (based on whether the request was download/plot) and sends back the resulting file over the network
b) a module that uses desispec utils to do file reading + slicing/selection, including stuff like mapping edr -> fuji. Basically, most knowledge of the DESI data model lives here.
c) a module to do complicated work with Spectra objects.
** Extensibility
Currently the flow looks like: someone accesses one of the three endpoints we specified -> we give them back either a FITS file or an image file.

At a high level, any other major features we think it's important to build into the API? For instance, some kind of web viewer for the plots, authentication, etc.?
Obviously I'll make sure the implementation is as extensible as possible, but if there are concrete extensions we want to accomodate I'll plan explicitly for those.
** Cacheing and Cleanup
Since our responses currently look like files (whether FITS or HTML or image), I was imagining we could save each response file to =$CACHE_DIR/<api-call-parameters>/<request timestamp>=.
That way when a new API call is made, we can just:
- Check if something exists at the relevant path. If not, proceed with building the file.
- Check if the existing thing is sufficiently recent (we can define an arbitrary cutoff). If not, delete the old file and proceed with building the new one.
We can also have a low-priority cron job to just =rm -rf $CACHE_DIR= run however often we want.
** Limiting Response Size
One way to do this is pagination, although this might be tedious since our data is FITS files and I'm not sure how `naturally' they paginate.

If we know the typical size of the data, we can just set a maximum for request parameters like the size of the =radius= param or the number of target ids, and add a validation check to send back a descriptive error on invalid requests.

* Endpoints
All endpoints specify a release first like `daily' or the special name of a release.
** Tile
=tile/<tile-id>/<fibers>=
Select a tile folder, latest date, extract the relevant PETAL file names based on the <fibers>. Read each file into a Spectra, keep the ones matching requested fibers, and =stack= them.
# TODO: Look at the structure of FITS files and petal files specifically. So do we need to flat-concat the files, or can we do clever things with pulling out individual tables.

** Targets
=targets/<target-ids>=
Read a FITS to get the Healpix, Survey, Program for each target. That is sufficient to get a file path to a FITS file for each target

Then, simply read each file using read_spectra, restrict to the relevant target_ids, and stack them all.
Do some grouping so targets with the same file are pulled in a single read

Some intricacies - the filename depends on the release sometimes, it seems. Annoying.
** Radec
=radec/<ra,dec,radius>=
So select a point in the sky and a radius around it.
The zall-pix-<release>.fits is the core metadata, it seems.
This one seems iffy, geometry required, some actual algorithms.

So we are given a point and radius, we are interested in all targets within that radius.

* Methods
** Download
** Plot
* Overview
The ideal separation looks like:
** Assemble the data
*** Find the right files
*** Implementation knowledge
What parts of what files to extract.
This is just a set of calls to =read_spectra= to extract relevant rows and blindly pass them on.

**** FITS files - select columns
**** Queries to apply to get rows
Applied in =read_spectra=
*** Consolidate
- Execute the `query' assembled by the domain knowledge
- Builds a Spectra object


** Download
- Write the spectra to a tmp file
** Plot
- Write the plot to a tmp file
** Serve
- Takes a file path and sends it back over the wire.
-
** Top-level
Top-level routine.
- Accepts params
- Checks cache for existing
- Save timestamp of request
- Dispatch to utility to actually build spectra
- Takes returned spectra. Either build a fits file or a plot.png from it, and put it in the right place in /cache
- Return the filename
* Details
** Error-handing
* Sqlite DB
- Stephen's suggestion is to maintain a sqlite DB that reflects the zall.fits for each `frozen' release.
- How would this work?
** Requirements
- Build a DB
- Mechanically, use python's internal model of fits as our intermediate
- Can we cheese? https://github.com/noaodatalab-user/fits2db
- Damn, last commit 2018. Maybe not.
- Possibly filter cols/rows
- Rebuild on-demand
- Periodically sync/sanity check
- Generic: Build from new releases as well.
- Ideally: Automated pipeline to build but that's far-future
- Fallback - if this fails or is ambiguous somehow, try to read straight from FITS
- Encapsulate the fallback - the actual endpoint handler should just call `get-stuff-from-zall' and have that do everything it can.
-
* TODO
- Set up venv
- Review tutorial
-


* Testing on NERSC
- Just run from the Jupyter notebook.
- Should have DESI access from my notebook
- They have SSH access.
- So ssh in to NERSC, clone the git repo I'm working with
- Also set up emacs CLI in there. Emacs is on, clone dotfiles, install doom
- Jupyterhub lets you do terminals as well, but eww.
* Parameter parsing
** Reading
- There have to be standards for this, come on.

** Ideas
- Define a ParameterObject union type?
- Union of one dataclass per endpoint.
- Empty class =ParameterObject=
- Subclasses for actual content
* Notes from the Slice/Dice
- read_spectra(filename)
- write_spectra(outfile, spectra)
- .num_spectra to check size
- desispec.io.find_file


We can do NP-style complex logical filters and maps
#+begin_src python
keep = spectra.fibermap['FIBER'] % 50 == 0
subset1 = spectra[keep]

focalplane_radius = np.sqrt(subset1.fibermap['MEAN_FIBER_X']**2 + subset1.fibermap['MEAN_FIBER_Y']**2)
subset1.fibermap['FOCALPLANE_RADIUS'] = focalplane_radius
#+end_src

#+begin_src python
zcat = fitsio.read(zcatfile, 'ZCATALOG', columns=('TARGETID', 'TILEID', 'LASTNIGHT', 'PETAL_LOC', 'SPECTYPE', 'Z', 'ZWARN', 'FLUX_G'))

for tileid, night, petal in np.unique(bright_qso_zcat['TILEID', 'LASTNIGHT', 'PETAL_LOC']):
    coaddfile = desispec.io.findfile('coadd', tile=tileid, night=night, spectrograph=petal,
                                     groupname='cumulative')
    spectra = desispec.io.read_spectra(coaddfile, targetids=bright_qso_zcat['TARGETID'])
    spectra_camcoadd = coadd_cameras(spectra)
    spectra_list.append(spectra_camcoadd)

#+end_src
* Docs
#+begin_src python
desispec.io.meta.findfile(filetype, night=None, expid=None, camera=None, tile=None, groupname=None, healpix=None, nside=64, band=None, spectrograph=None, survey=None, faprogram=None, rawdata_dir=None, specprod_dir=None, download=False, outdir=None, qaprod_dir=None, return_exists=False, readonly=False, logfile=False)[source]¶
#+end_src

* TODO <2023-10-18 Wed>
** Questions
- Should we do partial cacheing? No, we should not, it is simply not worth the effort.
- Overhead of reading RA/DEC?
*** How do we catch/log/report errors?
- No comments on webapp
- Inside the spectra module: Fail early, raise an exception. So looks go-style. So raise as early as possible, as informative as possible.
- For webapp: Send an error response to user, log it to stdout, raise an exception. Wrap this all in a function, somehow.
- Have  a func that does logging/etc. and returns an exception, and then =raise= that.

*** How to read Daily, without a zcatalog? Use the csv?
- Think about this later
- Look into sqlite
- RADEC can be done from csv kind of.
** Radec
- In principle I have a working impl, test it.
** Error-handling
- Catch user errors carefully
- For internal errors, have catches in the top-level handlers in the relevant module
- Catch errors, log them, respond with a traceback
** Testing - edge cases
*** How to interpret outputs for testing:
- Trivial: Check if it's pulled out the correct line
- Sanity-check unit tests: Run without crashing, produce nonempty files, etc.
- Do low-bar testing: Does it return any rows, etc.
- So do manual testing to check content, on bug write unit test that will catch it if it regresses, or anything.
- So catch breaking changes early
** Testing - more than sanity
** Webapp side
- All of this, really
** Sqlite idea
** Dockerization stuff
** Daily
** Optimisation
- Don't read the same file multiple times
- Ask: Overhead of reading RA/DEC?
** Cacheing
Should we do partial cacheing? No, we should not, it is simply not worth the effort.
* TODO <2023-10-19 Thu>
- Work on a user guide, send to Stephen over email and add to github.
- Corollary
- Next steps: RADEC + webapp
- Webapp testing:
* Security Design
- Ask Stephen about existing setups.
** File Access
Same docker image.
Have one running with DESIROOT mounted, and another running with DESIROOT/fuji, DESIROOT/iron only (so access to only public specprods).
Higher abstraction level: Python script to read from a text file of public release names and build a docker command out of it and spawn that process.
** Password Access
- By default, the script runs basic-auth required
- If we can do this, we can also do allowed-releases here and avoid running different docker images.
- So then we have a private one running with password-locked = True, and a public one with password-locked = False and releases set to public.
*** Config file
- DESIROOT location
- Allowed releases?
- Password locked: y/n
- Username and password (encoded/encrypted, will look into this)
- The file will be bundled into the docker image, so not something directly accessible from the filesystem.
* Security Design Email
Firstly, I wanted to ask about the existing setup - Anthony mentioned DESI had some existing stuff in place for handling security of private/public releases, and I wanted to ask if you could direct me to any docs/wikis which go over those, or anything in particular you think I should understand about them.

I was thinking of using Basic Auth for the password-protection, since it plays well with both automated access via API and manual via webpage.
It's similar to the screening used here: https://desi.lbl.gov/desipub/app/MembershipForm/form
We'd probably store an encrypted version of the password as storage somewhere in the docker image, and check the input password against that as our auth method. Flask has various add-ons and features that should make basic auth clean to implement (I found several tutorials online).
I'm not an expert on security stuff so I'm not sure if that meets whatever standards/expectations DESI has - if it doesn't, or if you have suggestions on what to do instead, let me know.
https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication#basic_authentication_scheme


One idea was to have a config file bundled in with the docker image that dictates whether the API requires username/password (and if so, includes the credentials encrypted somehow), as well as which releases it's allowed to access.
So then you'd have two copies of the image running - one with password protection and full file access, and another with no password protection and access only to public directories.

Another idea was to just have the config file handle password protection, and restrict file access by mounting different files. When we run the docker image we can mount parts of the local filesystem - the unrestricted API would mount DESIROOT, but the restricted one might only mount DESIROOT/fuji and DESIROOT/iron, for instance.
So then the `public' docker image has no idea the private files even exist, so any attempt to access them via a bug or anything will just fail with `file not found'.
We could build higher levels of abstraction over this, like a small script to generate the correct docker commands based on a list of public releases.
The advantage here is that the main codebase doesn't have to care about whether it's allowed to access a file, so we can separate out the access control into this layer, at the cost of giving us a new moving part to worry about.
* HW <2023-11-02 Thu>
- Check refactors, fix the ones that will be big problems later if any
- Focus: Build out webapp and test, including cacheing.
- Clone fuji
- Auth/etc. doesn't matter here. So this is just a dumb API server.
- Error handling
- Consider making the Parameters structs into plain dictionaries, so we can kwarg them. Do this
- Status: Tile endpoint works, produces a file, ran rudimentary cache checks.
- Next steps: More testing, plot
* Next Steps
** Error-handling
Build_Request and Build_Params need rich error-checking so we can feed info on malformed reqs back to user.

Catch errors on top level. Errors in build_request give 403, errors elsewhere give 500.

build_response: Check if the time-parsing fails.
Check if library calls fail, basically - write_spectra and plotspectra.
Also read_tile_spectra, fitsio.read,  read_spectra, Table.read

DONE Log reqs as they come in, and log meta like rebuilding/used cache
** Validation
Largely tedium. Validate parameters as specified
TODO ask about validation rules.
** Refactoring
* Actual dockering
- Mount DESIROOT somewhere
- Specify where DESIROOT is mounted (from POV of the image)
- Might just hardcode DESIROOT='/desiroot' in the python thing, and always mount $DESIROOT to target '/desiroot' in the run.sh
- Then we escape environment var juggling
** How does this play with the public multi-mount?
Mount the allowed releases as '/desiroot/<fuji>', etc.
It works fine.
* DONE Resp
- They would all have a similar pattern of turning the request url into formal parameters and then calling build_spectra.get_target_spectra, figured it made sense to do that in one func rather than 3.
- The basic idea is that then the webapp would have to care about this domain stuff, whereas ideally I think domain stuff will be concentrated in build_spectra.
- Will make the env var changes.

- TODO: What does it do to history.

- DONE fix bug with parse_list

- DONE use spectro_redux

- DONE debug: Builds html but returns as fits
* HW
- Error handling
- Docs
- Post reqs + optional query params in URL
** Post requests
Use JSON.
Have a filters sub-object and a params sub-object

How to do params properly?

In the POST, pass them via a dictionary.
Internally, how to store them? I prefer objects but dicts are fine.
Objects let us canonise.

So recapping, currently we read params as a slashed string, parse them into an object.

For POST requests, we need to read params in as a json/map.
High-level options are parse them into
- Object :: Urgh. The object structure will
- Map :: DESI people probably prefer maps to objects.

  Either way, most standard way to do things is to modify =build_params= so that either a) acts differently on maps vs strings or b) accepts another parameter =post?= which tells it how to behave.
  The first seems nicest. So that's that.
  Ok. So that's solved conceptually.
  Making it an object will be tedious, making a dictionary is just identity.
  Although if we want to introduce aliases for formal param names, dictionary and object are equally tedious.
  For now stick with object
  TODO: Ask Stephen about object vs dictionary.

** Optional Params
*** Dict
Then the trick becomes parsing them.

Basic filters are


Have a final function that takes in a spectra object (or a set of spectra objects) and a dictionary and interprets them via filtration.

Then we can formalise it as enums

So far we have a story for reading in filters and recalling them.
What remains is a story for doing stuff with them.
**** Preprocessing
Before we hand it off to endpoint handlers
This seems fine. We'll enforce the abstraction that filters should be applied individually and compose. So assoc/commute, in some sense.
So we can just have a thing that walks down the list of keys, and for each actually specified option, does a thing.
**** Within handlers
For endpoint-specific filters.
This'll involve adding more abstraction/modularity to the handlers, currently they're very do-one-thing.
TODO think about this.
**** Postprocessing
Unlikely, but for trimming out stuff after the handlers have read it in.
* Email <2024-01-17 Wed>
Hi Stephen,

I've been working on the API a bit. I'm about to test it but I've added some code so that POST requests should just work, and I've added some missing docstrings.

I wanted to ask about the filters feature you mentioned. Currently the code has hooks to read in these filters and save them as part of a request, but no way to act on them or have them modify the result.

As I'm imagining it, the two main kinds of filters (from an implementation perspective) are
a) Those that we apply after getting the normal result from the endpoint function. For instance, filtering out targets that satisfy a certain property or have a certain type, after reading in spectra/data for a specified list of targets.
b) Filters that change how we call the endpoint function. For instance, ~survey=main~ might translate into an additional parameter passed to our call to =desispec.io.read_tile_spectra=.

Firstly, I wanted to ask if there are other types/implementations of filters that I'm not thinking of.
Secondly, I wanted to ask how many filter options of type (b) you foresee there being - if it's a lot, we'll need to come up with a pattern to implement those, but if that's a rare case we could just implement it ad-hoc/case-by-case.
** Resp
For the filters, I think we'll use this for the radec endpoint, but not the tile or targets endpoint.  For the tile or targets endpoint, the user is already specifying a specific set of fibers/targets to load, so we just give it to them.  But the radec endpoint is more of a query where the user doesn't know ahead of time which targets might match the location cut, and they might want to also put other requirements on the matching targets.  Since reading the actual spectra is expensive, I think we should implement this mid-stream, after reading the redshift catalog (that we use for TARGET_RA, TARGET_DEC distance cuts), then apply any additional filters to trim down the target list before proceeding with reading the spectra/coadd files to get the actual spectra.  Does that make sense?

At minimum we should support equality filters (e.g. SURVEY='main'), but it could also be handy to support less-than/greater-than (e.g. Z>2).  If multiple filters are specified, I think it is ok to define those as meaning the logical AND of all the filters (i.e. a target has to pass all of them) and we don't need to invent the syntax to support both AND and OR.  That being said, if there is an industry standard way for specifying generic selection filters in a URL we could consider that.

Another gotcha is that for efficiency retrieve_targets only reads a subset of the columns in the zcatalog file, but the user might want to filter on other columns.  So I think we'll also need to check the filters for any non-default columns, and add those to the list to read.  If the user specifies a column that doesn't exist in the zcatalog file, return some informative error.

I think we should seek a semi-generic solution for filtering on any column in the zcatalog file instead of implementing them on a custom case-by-case basis.

Does that make sense?
*** Filters
So we can have arbitrary post-endpoint filters.

We can support 'only use targets with metadata property x' by translating those into filters in retrieve_targets, before we read any spectra


So the basic idea is that most filter logic should be concentrated in retrieve_targets.
What we can do is take the list of filters, add the keys/category names to the list of cols we read in, and then our targets exist as.
This also suggests that treating targets, etc. as dicts we can index into rather than strict objects is a much better idea.
Refactor time. Oh well.
So then filters contain the information "category, boolean expr". Using eval is suicidal of course and I don't hate myself that much.
**** Composing Filters
We only compose with AND.
So abstractly applying filters successively is fine.
**** Equality filters
Easy enough. The trick is translating values to floats/ints/whatever in a context-aware way.
**** Generic numerics
We only really need to support \ge, >, \le, <
Nice, right?

In POST requests, the filters sub-object would be flat with key-values looking like "category":">=5", "category":"=5" or just "category":"5", etc. So keys are categories, values are the boolean filter.
We can parse these without too much effort.
***** Abstracted parsing because we're normal
Split into operator and value
Dict that maps operators to binary boolean predicates
Curry lookup[operator] with the value to get a one-arg boolean func
Use that curried func as a filter, basically.
Alternatively, if numpy is weird, remember how numpy does index filtering and translate directly into numpy clauses or whatever weird stuff they do.
* Email
Hi Stephen,
Based on this, I have a rough idea for filter implementation - let me know if it seems over- or under- engineered in any way, or if you think it's good to start implementing/hacking about with.

- User supplies filters of the form ~<key> <operator> <value>~, where operator is one of =,>,<. For instance ~SURVEY=main~ or ~Z>2~ would both be valid filters.
- We translate/parse the filter into a numpy array mask, and represent the `combined filter' as a logical AND of array masks (numpy can do this nicely as far as I know)
- We also AND in our base clauses, like ~np.isin(zcat["TARGETID"], target_ids))~ to get our final ~keep~ mask.
- Then we just do ~zcat=zcat[keep]~ and process the zcat as normal

The second part of this that I'm still working on is adding metadata to the target objects we read in and send back. My basic plan is to:
- Refactor Target objects to be dictionaries so we can read in arbitrary metadata/columns
- When reading target metadata, read the basic fields, but also scan the keys of the filters passed in, and add those keys to the cols read in by ~fitsio.read~, and also add them to our target objects
- We'd do some minimal canonisation such as enforcing consistent case - ideally the filter option titles will match up with the names we use for columns internally.
- After that, I'm not sure exactly what we'd do with that additional metadata - we could mainly use it for post-processing filters, I imagine. Not sure about how sending it back to the user would work - if we want to do anything like that, I imagine we'd have to embed the metadata in a Spectra object somehow?

#+begin_src python
def func(key, operator, value):
    opfunc = "func based on string val of operator"
    return lambda target: opfunc(target[key],value)

def mask(key, op, val):
    fn = func(key, op, val)
    return lambda arr: arr[fn(arr)]

def compose_masks(arr, filters):
   masks = tuple(mask(*f) for f in filters)
   return arr[np.logical_and(*masks)]
#+end_src
* TODO <2024-01-29 Mon>
- Refactor to have Targets be dataframe
- Build up the filter system in =retrieve_targets=.
** Catalogue Endpoint
- All existing endpoints get nested under spectra, so that BASE/<specprod/spectra/<endpoint/<args>
- New endpoint /zcat/<download|plot>/<endpoint>/<args>
- Does a dry run, read metadata of targets that would be accessed (via retrieve targets, probably) and spits out as a fits file.
- Eventually make it also send html tables if asked, but later.
- Longterm feature: Metadata catalogue would like multiple download options.
** <2024-02-05 Mon>
$Z$ (redshift) is an int.
So use that to filter on integers.

ZCAT endpoint is the focus.

Doing an OR filter would be nice.
So more general filtering.

Something like a minimal filter.
So basically instead of hitting an endpoint, they just say `give me all things matching this set of clauses', they locally do OR filtering, etc. and give us back target IDs.

So abstract


Oddity of design: Currently the only way to get a non-default column is to filter on it.

Context:
~100 columns
~10 that we currently return.
There's a 20-30 that are the main, others are niche.

Possibility 1: Send back 20-30, fuck you if its overkill
Possibility 2: Send back 10, and person can request others ad-hoc by filtering

Possibility 3: No-op filter. Possibility "?" instead of ">,<,=" as an operator. So "PROGRAM ?" corresponds to a trivial "return True" filter, and just adds it to the list of cols to read.

Open Question: DevDocs.
TODO: Start drafting it, recall Kapstan docs.


TODO: Move official external stuff: README and devdocs, userdocs, to Markdown.
* <2024-02-12 Mon>
- Finish devdocs
- Zcat endpoint
- Clean up repo: Pycache, etc.
- Catalog: fitsio.read -> fitsio.write
- Catalog restrictions: For getting back a file, no upper limits
- For views: How do we do that? Do it based on the HTML table.
- Principles: Third party is ok, take something officially maintained.
- TODO: Check if MUI table works. Just do a flat HTML table.
- Add a cutoff, beyond that point send back an info page on the FITS endpoint, so they can do stuff with it.
- TODO: Devdocs, zcat endpoint.
** Zcat endpoint:
- Backend: Fill in the gaps
- Backend: Validation
- FE: Look into

** Data
tilecumulative zcat data shape
        dtype=[('TARGETID', '>i8'), ('SURVEY', '<U7'), ('PROGRAM', '<U6'), ('LASTNIGHT', '>i4'), ('SPGRPVAL', '>i4'), ('Z', '>f8'), ('ZERR', '>f8'), ('ZWARN', '>i8'), ('CHI2', '>f8'), ('COEFF', '>f8', (10,)), ('NPIXELS', '>i8'), ('SPECTYPE', '<U6'), ('SUBTYPE', '<U20'), ('NCOEFF', '>i8'), ('DELTACHI2', '>f8'), ('PETAL_LOC', '>i2'), ('DEVICE_LOC', '>i4'), ('LOCATION', '>i8'), ('FIBER', '>i4'), ('COADD_FIBERSTATUS', '>i4'), ('TARGET_RA', '>f8'), ('TARGET_DEC', '>f8'), ('PMRA', '>f4'), ('PMDEC', '>f4'), ('REF_EPOCH', '>f4'), ('LAMBDA_REF', '>f4'), ('FA_TARGET', '>i8'), ('FA_TYPE', 'u1'), ('OBJTYPE', '<U3'), ('FIBERASSIGN_X', '>f4'), ('FIBERASSIGN_Y', '>f4'), ('PRIORITY', '>i4'), ('SUBPRIORITY', '>f8'), ('OBSCONDITIONS', '>i4'), ('RELEASE', '>i2'), ('BRICKNAME', '<U8'), ('BRICKID', '>i4'), ('BRICK_OBJID', '>i4'), ('MORPHTYPE', '<U4'), ('EBV', '>f4'), ('FLUX_G', '>f4'), ('FLUX_R', '>f4'), ('FLUX_Z', '>f4'), ('FLUX_W1', '>f4'), ('FLUX_W2', '>f4'), ('FLUX_IVAR_G', '>f4'), ('FLUX_IVAR_R', '>f4'), ('FLUX_IVAR_Z', '>f4'), ('FLUX_IVAR_W1', '>f4'), ('FLUX_IVAR_W2', '>f4'), ('FIBERFLUX_G', '>f4'), ('FIBERFLUX_R', '>f4'), ('FIBERFLUX_Z', '>f4'), ('FIBERTOTFLUX_G', '>f4'), ('FIBERTOTFLUX_R', '>f4'), ('FIBERTOTFLUX_Z', '>f4'), ('MASKBITS', '>i2'), ('SERSIC', '>f4'), ('SHAPE_R', '>f4'), ('SHAPE_E1', '>f4'), ('SHAPE_E2', '>f4'), ('REF_ID', '>i8'), ('REF_CAT', '<U2'), ('GAIA_PHOT_G_MEAN_MAG', '>f4'), ('GAIA_PHOT_BP_MEAN_MAG', '>f4'), ('GAIA_PHOT_RP_MEAN_MAG', '>f4'), ('PARALLAX', '>f4'), ('PHOTSYS', '<U1'), ('PRIORITY_INIT', '>i8'), ('NUMOBS_INIT', '>i8'), ('CMX_TARGET', '>i8'), ('DESI_TARGET', '>i8'), ('BGS_TARGET', '>i8'), ('MWS_TARGET', '>i8'), ('SCND_TARGET', '>i8'), ('SV1_DESI_TARGET', '>i8'), ('SV1_BGS_TARGET', '>i8'), ('SV1_MWS_TARGET', '>i8'), ('SV1_SCND_TARGET', '>i8'), ('SV2_DESI_TARGET', '>i8'), ('SV2_BGS_TARGET', '>i8'), ('SV2_MWS_TARGET', '>i8'), ('SV2_SCND_TARGET', '>i8'), ('SV3_DESI_TARGET', '>i8'), ('SV3_BGS_TARGET', '>i8'), ('SV3_MWS_TARGET', '>i8'), ('SV3_SCND_TARGET', '>i8'), ('PLATE_RA', '>f8'), ('PLATE_DEC', '>f8'), ('TILEID', '>i4'), ('COADD_NUMEXP', '>i2'), ('COADD_EXPTIME', '>f4'), ('COADD_NUMNIGHT', '>i2'), ('COADD_NUMTILE', '>i2'), ￼ Favours: Constables or ￼ Favours: The Church can be obtained reliably 3-at-a-time, by breeding the ￼ Hound of Heaven at the Labyrinth of Tigers. This requires repeated Zee trips to capture ￼ Plated Seals, and is recommended for ￼ Monster-Hunters as they can get an extra reward from each trip.
('MEAN_DELTA_X', '>f4'), ('RMS_DELTA_X', '>f4'), ('MEAN_DELTA_Y', '>f4'), ('RMS_DELTA_Y', '>f4'), ('MEAN_FIBER_RA', '>f8'), ('STD_FIBER_RA', '>f4'), ('MEAN_FIBER_DEC', '>f8'), ('STD_FIBER_DEC', '>f4'), ('MEAN_PSF_TO_FIBER_SPECFLUX', '>f4'), ('MEAN_FIBER_X', '>f4'), ('MEAN_FIBER_Y', '>f4'), ('TSNR2_GPBDARK_B', '>f4'), ('TSNR2_ELG_B', '>f4'), ('TSNR2_GPBBRIGHT_B', '>f4'), ('TSNR2_LYA_B', '>f4'), ('TSNR2_BGS_B', '>f4'), ('TSNR2_GPBBACKUP_B', '>f4'), ('TSNR2_QSO_B', '>f4'), ('TSNR2_LRG_B', '>f4'), ('TSNR2_GPBDARK_R', '>f4'), ('TSNR2_ELG_R', '>f4'), ('TSNR2_GPBBRIGHT_R', '>f4'), ('TSNR2_LYA_R', '>f4'), ('TSNR2_BGS_R', '>f4'), ('TSNR2_GPBBACKUP_R', '>f4'), ('TSNR2_QSO_R', '>f4'), ('TSNR2_LRG_R', '>f4'), ('TSNR2_GPBDARK_Z', '>f4'), ('TSNR2_ELG_Z', '>f4'), ('TSNR2_GPBBRIGHT_Z', '>f4'), ('TSNR2_LYA_Z', '>f4'), ('TSNR2_BGS_Z', '>f4'), ('TSNR2_GPBBACKUP_Z', '>f4'), ('TSNR2_QSO_Z', '>f4'), ('TSNR2_LRG_Z', '>f4'), ('TSNR2_GPBDARK', '>f4'), ('TSNR2_ELG', '>f4'), ('TSNR2_GPBBRIGHT', '>f4'), ('TSNR2_LYA', '>f4'), ('TSNR2_BGS', '>f4'), ('TSNR2_GPBBACKUP', '>f4'), ('TSNR2_QSO', '>f4'), ('TSNR2_LRG', '>f4'), ('SV_NSPEC', '>i4'), ('SV_PRIMARY', '?'), ('ZCAT_NSPEC', '>i8'), ('ZCAT_PRIMARY', '?')])

  
* Updates <2024-02-19 Mon>
Hi Stephen,

Here are the updates for what I worked on this week:

- Finished a first cut implementation of the Zcat endpoint, including some basic testing of tile/target zcat endpoints which worked smoothly.
- Did a bunch of refactoring and restructuring, so functions are grouped/arranged properly, and also reorganised the file structure a little.
- Completed the template for devdocs and userdocs in markdown, and fleshed out the main sections (although some still remain)

  I looked into some options for the GUI table. Depending on how important this feature is to our intended design, my instinct would be to deprioritise it and focus more on things like more varied endpoints, features in the vein of filtering, etc, since having a sophisticated GUI might be more trouble than it's worth at the moment (at least compared to the other ways to spend time/effort).

  Obviously I don't know how useful/demanded such a feature is, so I may be completely off-base here. Either way, I've added highlights on the two most viable options - MUI and DataTables - that I looked at.

*** MUI
It's built on top of React.js, which is a slightly heavy framework but it is very nice for building slick UIs.

It has a component called =DataGrid= which is designed for displaying complex tables in the browser and letting the user do interactive sorts, filters, etc. on that data from the browser itself.

It's been a fairly long-running project, almost as old as React itself.
*** DataTables
The main draw of this is that it's built mainly on top of plain HTML/JS. The only library it uses is jQuery.
* Working with the Cache
** Minimal/Hacky
** Full Config
Basically, have a config file locally, mounted alongside the desi data.
Can we use TOML? Please? Please? Please?

Lets you set most of what are currently constants

Config file location: Hardcode for now.

Replace to read from TOML, else use defaults.

Recall cache structure.

<request>/<timestamp>.<ext>


How to handle config efficiently?
Read config using lib. Store it in global var CONFIG.
replace global vars with calls CONFIG[???]. Done.


Read the file on startup, store the value in a global var CONFIG

** Better cleaning
Build a more complex cache clean routine. Every hour, kill everything with timestamp older than an hour ago. So we only ever store the last 60 minutes of requests, but we always store those, and never delete a request that's less than 60 minutes old.
Probably nicer.
* TODO <2024-02-26 Mon>
Check that we can read back a FITSIO file from the cache, since angle brackets in filenames are parsed weirdly. It can, victory!

Config file should be changeable via CLI option. TODO

Bundle a =default.toml= with the container, have that be the hardcoded fallback conf file.

For cache cleaning: Look for access time, rather than creation time and delete based on that cutoff.
Recall we don't care about time to thing.

For dailies: We could have a cron job to rm -rf all daily files every 24 hours.

Every hour, if the cache dir is scary big, nuke it.
* Mail
Hi Stephen,

I've run into an error with the ="spectra/plot/fujilite/radec/210.9,24.8,180"= endpoint, which I've narrowed down to a bug in how my code interacts with =prospect.viewer.plot_spectra=.

Specifically, the Plot endpoint fails, but if I try the same request with Download instead it works fine, so I'm pretty sure the bug is specifically related to =plotspectra=.

I've attached the relevant function call (this is the only time I call =plotspectra= in my code), as well as the relevant part of the traceback. My guess is that the naive call to =plotspectra=  asks it to plot a target of type =QSO=, and either =plotspectra= doesn't know how to draw those or it isn't able to find the template/specifications for drawing them.

#+begin_src python
plotspectra(
            spectra,
            zcatalog=spectra.extra_catalog,
            html_dir=save_dir,
            title=file_name,
            with_vi_widgets=False,
            with_full_2ndfit=False,
            num_approx_fits=0,
        )

#+end_src

#+begin_src
  File "/home/vivien/OneDrive_Personal/work/urap/desi-api/webapp.py", line 282, in spectra_to_html
    plotspectra(
  File "/home/vivien/.local/lib/python3.11/site-packages/prospect-1.3.1.dev536-py3.11.egg/prospect/viewer/__init__.py", line 275, in plotspectra
    model = create_model(spectra, zcatalog,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vivien/.local/lib/python3.11/site-packages/prospect-1.3.1.dev536-py3.11.egg/prospect/viewer/__init__.py", line 109, in create_model
    tx    = templates[(zb['SPECTYPE'], subtype)]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyError: ('QSO', '')
#+end_src
